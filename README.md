# Learning to Predict in an Artificial Language

This is an eyetracking experiment testing whether people learning can artificial language can make use of detailed acoustic information to predict upcoming input.

Participants are exposed to an artificial language in a visual-world task. 
The language is based on English, but crucially differs in some initial consonants.
After the exposure phase, I use eyetracking to measure whether participants utilize anticipatory coarticulation for prediction in a way consistent with the artificial language.

A detailed description is given in the docs directory.

